{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reconocimiento Facial Y Detector De Mentiras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7mg/n+qJ2AFTnr5rthnWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b878db24636047ca99a6bfd9e7fdd4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34cb1df079734b699e3e5ee75a0982a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95eb67dfa563452ca247d5a82f19432e",
              "IPY_MODEL_d9c6ac2d653b46f6bed02f7dba5206bc"
            ]
          }
        },
        "34cb1df079734b699e3e5ee75a0982a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95eb67dfa563452ca247d5a82f19432e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e89ff006e20d4bb29154a39d5f35e04c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a889e2e567f1472f8b87aff160514c5a"
          }
        },
        "d9c6ac2d653b46f6bed02f7dba5206bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a5a2197dd794812b5e554b5782eb1b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:00&lt;00:00, 4138.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1df4564e1cc14d9e9e3c06a68cc1073a"
          }
        },
        "e89ff006e20d4bb29154a39d5f35e04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a889e2e567f1472f8b87aff160514c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a5a2197dd794812b5e554b5782eb1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1df4564e1cc14d9e9e3c06a68cc1073a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank14/Reconocimiento_Facial_Y_Detector_De_Mentiras/blob/main/Reconocimiento_Facial_Y_Detector_De_Mentiras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hnQ5GxAvAzj"
      },
      "source": [
        "# **Sistema detector de mentiras**\n",
        "\n",
        "El presente proyecto utiliza OpenCV y Haar Cascade para la detección y clasificación de rostros.\n",
        "\n",
        "Para el reconocimiento facial emplearemos: Eigenfaces y el Local Binary Patterns Histograms.\n",
        "\n",
        "Y finalmente para la detección de mentiras se usan los modelos pre-entredados para detectar las pupilas y los ojos para determinar por medio de un simple cálculo, donde dependiendo la posición de los ojos en cuanto a la posición inicial menos la final la posibilidad de estar mintiendo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PIwMdv5jzz4"
      },
      "source": [
        "# **Acceso a la cámara web para obtener imágenes y vídeo**\n",
        "\n",
        "Este cuaderno repasará cómo acceder y ejecutar código en imágenes y vídeos tomados con tu webcam.\n",
        "\n",
        "Para el propósito de este proyecto vamos a utilizar **Haar Cascade** de **OpenCV** para hacer la detección de rostros capturados a través de la cámara de nuestra computadora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN99l9wff_Yb"
      },
      "source": [
        "# Importar dependencias\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from time import time\n",
        "import datetime\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hNpJ4gk2Cq"
      },
      "source": [
        "# **Funciones de ayuda**\n",
        "\n",
        "A continuación se presentan algunas funciones de ayuda para realizar la conversión entre diferentes tipos y formatos de datos de imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PJHsiQPlCNH"
      },
      "source": [
        "# Función para convertir el objeto JavaScript en una imagen OpenCV\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: Objeto JavaScript que contiene la imagen de la cámara web\n",
        "  Returns:\n",
        "          img: OpenCV BGR imagen\n",
        "  \"\"\"\n",
        "  # Decodificar imagen base64\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # Convertir bytes a array numpy\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype = np.uint8)\n",
        "  # Decodificar la matriz numpy en la imagen BGR de OpenCV\n",
        "  img = cv2.imdecode(jpg_as_np, flags = 1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# Función para convertir la imagen del cuadro delimitador del rectángulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vídeo\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (píxeles) que contiene el rectángulo a superponer en el flujo de vídeo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "# Función para convertir la imagen del cuadro del ojo izquierdo delimitador del rectángulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vídeo\n",
        "def bbox_eye_left_to_bytes(bbox_array_eye_left):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (píxeles) que contiene el rectángulo a superponer en el flujo de vídeo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_eye_left_PIL = PIL.Image.fromarray(bbox_array_eye_left, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_eye_left_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes_eye_left = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes_eye_left\n",
        "\n",
        "# Función para convertir la imagen del cuadro del ojo derecho delimitador del rectángulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vídeo\n",
        "def bbox_eye_right_to_bytes(bbox_array_eye_right):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (píxeles) que contiene el rectángulo a superponer en el flujo de vídeo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_eye_right_PIL = PIL.Image.fromarray(bbox_array_eye_right, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_eye_right_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes_eye_right = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes_eye_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91e2NgVEmiDO"
      },
      "source": [
        "# **Haar Cascade Classifier**\n",
        "\n",
        "Para este proyecto vamos a ejecutar un algoritmo simple de detección de objetos llamado **Haar Cascade** en nuestras imágenes y vídeo obtenido de nuestra cámara web. **OpenCV** tiene un modelo de detección de rostros pre-entrenado en **Haar Cascade**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzwYU1dm-3L"
      },
      "source": [
        "# Inicializar el modelo de detección de rostros y ojos en Haar Cascade\n",
        "# Entre mejor sean los modelos, mejor serán los resultados\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'))\n",
        "eye_left_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_lefteye_2splits.xml'))\n",
        "eye_right_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_righteye_2splits.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02b-1H0vnZq8"
      },
      "source": [
        "# **Imágenes de la cámara web**\n",
        "\n",
        "Ejecutar código en imágenes tomadas de la cámara web es bastante sencillo. Utilizaremos código dentro de **Google Colab's Code Snippets** que tiene una variedad de funciones de código útiles para realizar varias tareas.\n",
        "\n",
        "Utilizaremos el fragmento de código de **Camera Capture** para utilizar la cámara web de tu computadora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OJ72vZSn396"
      },
      "source": [
        "def take_photo(filename = 'photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Tomar Foto';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Redimensiona la salida para que se ajuste al elemento de vídeo.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Espere a que se haga clic en Capturar.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # Obtener datos fotográficos\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # Obtener una imagen en formato OpenCV\n",
        "  img = js_to_image(data) \n",
        "  # Imagen en escala de grises\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # Obtener las coordenadas de la caja delimitadora de la cara mediante la cascada de Haar\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # Dibujar el cuadro delimitador de la cara en la imagen\n",
        "  for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(128, 255, 0),2)\n",
        "  # Guardar la imagen\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X74gYZZeS4kt"
      },
      "source": [
        "## **Ejecutar la captura con la camara**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BAfGdvdoPoX"
      },
      "source": [
        "try:\n",
        "  # Nombre del archivo a guardar\n",
        "  filename = take_photo('photo.jpg')\n",
        "  # Mostrando por consola el nombre del archivo capturado\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Muestra la imagen que se acaba de tomar\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Se lanzarán errores si el usuario no tiene una cámara web \n",
        "  # O si no concede el permiso acceso al uso de la misma\n",
        "  print('Se presenta el siguiente error: ' + str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXn5HNmSqzZ2"
      },
      "source": [
        "# **Vídeo de cámara web**\n",
        "\n",
        "Ejecutar el código en el vídeo de la cámara web es un poco más complejo que las imágenes. Tenemos que iniciar un flujo de vídeo utilizando nuestra cámara web como entrada. A continuación, ejecutamos cada fotograma a través de nuestro programa **(detección de rostros)** y creamos una imagen superpuesta que contiene el cuadro delimitador de la detección. A continuación, superponemos la imagen del cuadro delimitador en el siguiente fotograma de nuestro flujo de vídeo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giNPTpJPsXz3"
      },
      "source": [
        "# JavaScript para crear correctamente nuestro flujo de vídeo en directo utilizando nuestra cámara web como entrada\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No existen datos';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: green; font-weight: bold;\">' +\n",
        "          'Cuando haya terminado, haga clic aquí o en el vídeo para detener esta demostración</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNRKJ6aW22E"
      },
      "source": [
        "## **Iniciar la transmisión de vídeo desde la cámara web**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dADWXnDqs_3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1fc7a757-c975-42e2-a11d-b8e4ae872b14"
      },
      "source": [
        "# Iniciar la transmisión de vídeo desde la cámara web\n",
        "video_stream()\n",
        "# Etiqueta para el vídeo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que esté vacío\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposición transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "\n",
        "    # Imagen en escala de grises para la detección de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la región de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposición\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(128,255,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # Convertir la superposición de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposición\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No existen datos';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: green; font-weight: bold;\">' +\n",
              "          'Cuando haya terminado, haga clic aquí o en el vídeo para detener esta demostración</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzhKqyPtvRK"
      },
      "source": [
        "# **Reconocimiento Facial**\n",
        "\n",
        "Para poder llevar a cabo el reconocimiento facial, en primer lugar necesitaremos recolectar los datos, es decir los rostros de las personas que se desee reconocer, posteriormente entrenaremos el clasificador, para finalmente probarlo. Para todo este proceso será necesario usar la  detección de rostros con **haarcascades** que habíamos visto anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu-UM-btthVw"
      },
      "source": [
        "## **Almacenar datos de personas manualmente**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKIWFA9L7kvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fde069-9928-486b-c11c-eb0712f24da6"
      },
      "source": [
        "# Se solicita el nombre de la persona a la que se realizará el reconocimiento\n",
        "personName = input('Nombre de la persona a reconocer: ')\n",
        "# Se crea la carpeta donde se almacenaran las imagenes\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "personPath = dataPath + '/' + personName\n",
        "# Si no existe la carpeta se crea\n",
        "if not os.path.exists(personPath):\n",
        "  print('Carpeta creada: ',personPath)\n",
        "  os.makedirs(personPath)\n",
        "\n",
        "def take_photo(filename = dataPath + '/' + personName, quality=0.8):\n",
        "  js = Javascript('''\n",
        "    function delay(n){\n",
        "    return new Promise(function(resolve){\n",
        "        setTimeout(resolve,n*1000);\n",
        "    });\n",
        "    } \n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Redimensiona la salida para que se ajuste al elemento de vídeo.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Espera de 5 segundos para que el obturador capture correctamente la imagen.\n",
        "      await delay(5);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # Obtener datos fotográficos\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # Obtener una imagen en formato OpenCV\n",
        "  img = js_to_image(data) \n",
        "  # Imagen en escala de grises\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # Obtener las coordenadas de la caja delimitadora de la cara mediante la cascada de Haar\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # Dibujar el cuadro delimitador de la cara en la imagen\n",
        "  for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(128, 255, 0),2)\n",
        "  # Guardar la imagen\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de la persona a reconocer: Francisco\n",
            "Carpeta creada:  Reconocimiento Facial/Data/Francisco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVME6VFtBG58"
      },
      "source": [
        "# Habilitar capturas por video\n",
        "cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
        "# Inicializando el contador para colocar diferentes nombres en las imágenes\n",
        "count = 0\n",
        "# Se conculta al usuario la cantidad de fotos a tomar se recomiendan pocas para pruebas rápidas\n",
        "photo = int(input('Ingrese la cantidad de imagenes a capturar: '))\n",
        "# Se ejecutara el ciclo según la cantidad indicada por el usuario\n",
        "\n",
        "while True:\n",
        "  # Leyendo las capturas\n",
        "  ret, frame = cap.read()\n",
        "  try:\n",
        "    # Nombre y ruta para almacenar las imágenes de entrenamiento\n",
        "    filename = take_photo(personPath + '/' + 'rostro_{}.jpg'.format(count + 1))\n",
        "    # Incremetando en 1 el contador por cada imágen capturada\n",
        "    count = count + 1\n",
        "    # Mostrando por consola el nombre del archivo capturado\n",
        "    print('Saved to {}'.format(filename))\n",
        "    # Tiempo de espera de 1 ms entre foto y foto\n",
        "    k =  cv2.waitKey(1)\n",
        "    # Condición para finalizar el ciclo\n",
        "    if k == 27 or count >= photo:\n",
        "        break\n",
        "    # Muestra la imagen que se acaba de tomar\n",
        "    display(Image(filename))\n",
        "  except Exception as err:\n",
        "    # Se lanzarán errores si el usuario no tiene una cámara web \n",
        "    # O si no concede el permiso acceso al uso de la misma\n",
        "    print('Se presenta el siguiente error: ' + str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgCVJE1ptZ38"
      },
      "source": [
        "## **Descargar datos de reconocimiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "b878db24636047ca99a6bfd9e7fdd4f4",
            "34cb1df079734b699e3e5ee75a0982a8",
            "95eb67dfa563452ca247d5a82f19432e",
            "d9c6ac2d653b46f6bed02f7dba5206bc",
            "e89ff006e20d4bb29154a39d5f35e04c",
            "a889e2e567f1472f8b87aff160514c5a",
            "9a5a2197dd794812b5e554b5782eb1b1",
            "1df4564e1cc14d9e9e3c06a68cc1073a"
          ]
        },
        "id": "MS_TUKmJtXXo",
        "outputId": "ee408a78-23ca-48c0-bb1e-35a04a583f48"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from multiprocessing.pool import ThreadPool\n",
        "\n",
        "def url_response(url):\n",
        "    path, url = url\n",
        "    r = requests.get(url, stream = True)\n",
        "    with open(path, 'wb') as f:\n",
        "        for ch in r:\n",
        "            f.write(ch)\n",
        "\n",
        "# Se solicita el nombre de la persona a la que se realizará el reconocimiento\n",
        "personName = input('Nombre de la persona a reconocer: ')\n",
        "# Se crea la carpeta donde se almacenaran las imagenes\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "personPath = dataPath + '/' + personName + '/'\n",
        "# Si no existe la carpeta se crea\n",
        "if not os.path.exists(personPath):\n",
        "  print('Carpeta creada: ',personPath)\n",
        "  os.makedirs(personPath)\n",
        "\n",
        "urls = [(personPath + \"Event1\", \"https://www.python.org/events/python-events/805/\"),\n",
        "(personPath + \"Event2\", \"https://www.python.org/events/python-events/801/\"),\n",
        "(personPath + \"Event3\", \"https://www.python.org/events/python-events/790/\"),\n",
        "(personPath + \"Event4\", \"https://www.python.org/events/python-events/798/\"),\n",
        "(personPath + \"Event5\", \"https://www.python.org/events/python-events/807/\"),\n",
        "(personPath + \"Event6\", \"https://www.python.org/events/python-events/807/\"),\n",
        "(personPath + \"Event7\", \"https://www.python.org/events/python-events/757/\"),\n",
        "(personPath + \"Event8\", \"https://www.python.org/events/python-user-group/816/\")]\n",
        "\n",
        "ThreadPool(9).imap_unordered(url_response, urls)\n",
        "\n",
        "print(\"Descargando imagenes...\")\n",
        "for i in tqdm(range(1000)):\n",
        "  print(\"\", end = '\\r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de la persona a reconocer: Lorena\n",
            "Carpeta creada:  Reconocimiento Facial/Data/Lorena/\n",
            "Descargando imagenes...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b878db24636047ca99a6bfd9e7fdd4f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUdHMsAmFkBj"
      },
      "source": [
        "## **Preparando Los Datos Para Entrenar**\n",
        "\n",
        "Antes de proceder con el entrenamiento es necesario tener cada una de las imágenes con una etiqueta asociada a la persona a la que pertenecen los rostros. Por ello, por ejemplo cuando leamos la carpeta **‘Persona 1’** todas esas imágenes se les asignará etiqueta 0, luego a todas imágenes de los rostros de **‘Persona 2’** se asignará 1,  de **‘Persona 3’** se asignará 2, y así sucesivamente. Con cada una de estas etiquetas le haremos saber al computador que las imágenes le corresponden a personas diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFAx7AjqF8g5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6411374a-223b-45ef-8dc0-6db43eaa59c8"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Se listan las personas por carpetas\n",
        "peopleList = os.listdir(dataPath)\n",
        "# Se imprime el listado de personas en el sistema\n",
        "print('Lista de personas: ', peopleList)\n",
        "# Declaramos labels en este se almacenarán las etiquetas correspondientes a cada imagen según la persona\n",
        "labels = []\n",
        "# Declaramos facesData en donde se almacenará cada una de las imágenes de los rostros\n",
        "facesData = []\n",
        "# Estableceremos un contador label en 0, para conforme se termine de leer las imágenes de una persona, cambie a otro valor. Esto ayudará al clasificador a entender que se tarta de diferentes personas\n",
        "label = 0\n",
        "for nameDir in peopleList:\n",
        "    personPath = dataPath + '/' + nameDir\n",
        "    print('Leyendo las imágenes')\n",
        "    for fileName in os.listdir(personPath):\n",
        "        print('Rostros: ', nameDir + '/' + fileName)\n",
        "        labels.append(label)\n",
        "        facesData.append(cv2.imread(personPath+'/'+fileName,0))\n",
        "    label = label + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lista de personas:  ['RaulEvent1', '.ipynb_checkpoints', 'RaulEvent6', 'RaulEvent7', 'Raul', 'RaulEvent3', 'RaulEvent4', 'RaulEvent8', 'RaulEvent2', 'Francisco', 'RaulEvent5', 'Lorena']\n",
            "Leyendo las imágenes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-8a412abba04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpersonPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnameDir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Leyendo las imágenes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rostros: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnameDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'Reconocimiento Facial/Data/RaulEvent1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfZftZEZLvES"
      },
      "source": [
        "## **Métodos para entrenar el reconocedor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLEvPsgg8L4y"
      },
      "source": [
        "#@title **Seleccione una de las siguientes opciones:** { display-mode: \"form\", run: \"auto\"}\n",
        "opciones = \"EigenFaces\" #@param [\"EigenFaces\", \"LBPH\"]\n",
        "\n",
        "print(\"La opción seleccionada fue: \" + opciones)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnic2jbRMNva"
      },
      "source": [
        "# Métodos para entrenar el reconocedor\n",
        "# Estos métodos crean la instancia, escriben y leen el set de datos para poder reconocer las imagenes solicitadas\n",
        "if opciones == \"EigenFaces\":\n",
        "  face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
        "  face_recognizer.write('modeloEigenFace.xml')\n",
        "  face_recognizer.read('modeloEigenFace.xml')\n",
        "  certain = 5700\n",
        "if opciones == \"LBPH\":\n",
        "  face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "  face_recognizer.write('modeloLBPHFace.xml')\n",
        "  face_recognizer.read('modeloLBPHFace.xml')\n",
        "  certain = 70\n",
        "else:\n",
        "  print(\"La opción seleccionada no es valida\")\n",
        "\n",
        "# Entrenando el reconocedor de rostros\n",
        "face_recognizer.train(facesData, np.array(labels))\n",
        "\n",
        "# Barra de carga para una espera más agradable\n",
        "print(\"Entrenando...\")\n",
        "for i in tqdm(range(10000)):\n",
        "  print(\"\", end = '\\r')\n",
        "print(\"Entrenamiento completado al 100%\")\n",
        "print()\n",
        "print(\"Modelo almacenado exitosamente\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSOwIAMsM3EV"
      },
      "source": [
        "## **Probando EigenFaces y LBPH para el reconocimiento facial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18SEo30NAd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5cf097e4-75c4-4397-d83f-e55093211b80"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Mostrar el nombre de la persona solamente\n",
        "imagePaths = os.listdir(dataPath)\n",
        "name = imagePaths[0]\n",
        "\n",
        "# Iniciar la transmisión de vídeo desde la cámara web\n",
        "video_stream()\n",
        "# Etiqueta para el vídeo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que esté vacío\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposición transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "\n",
        "    # Imagen en escala de grises para la detección de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la región de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposición\n",
        "    for (x,y,w,h) in faces:\n",
        "      # Obtiene el área donde se ubica el rostro \n",
        "      result = face_recognizer.predict(gray)\n",
        "      # Muestra el resultado en una escala definida para saber que tan probable es que ese sea el rostro de la persona almacenada\n",
        "      cv2.putText(bbox_array,'{}'.format(result),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
        "      # Si está dentro del rango de certeza muestra el nombre de la persona que se considera puede ser\n",
        "      if result[1] < certain:        \n",
        "          cv2.putText(bbox_array,'{}'.format(name),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      # Si no está dentro del rango simplemente lo muestra como persona desconocida\n",
        "      else:  \n",
        "          cv2.putText(bbox_array,'Desconocido',(x,y-20),2,0.8,(219,68,55),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(219,68,55),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # Convertir la superposición de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposición\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No existen datos';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: green; font-weight: bold;\">' +\n",
              "          'Cuando haya terminado, haga clic aquí o en el vídeo para detener esta demostración</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQCiYAUo00eC"
      },
      "source": [
        "---\n",
        "# **Detección de mentiras**\n",
        "\n",
        "Para este proyecto el objetivo fue detectar las iris de las personas, esto con el fin de detectar mentiras según la posición.\n",
        "\n",
        "\"*Los ojos nos hablan, pero a veces no sabemos interpretar lo que nos están diciendo. Con un entrenamiento adecuado podremos detectar hasta el más sutil de los detalles.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XelhmeRvt3Lu"
      },
      "source": [
        "# **Detección De Ojos**\n",
        "\n",
        "Para la detección de ojos simplemente reutilizamos el código anterior del video y de funciones de ayuda para que el recuadro de los ojos se guarde como una imagen los cuales serán capturados con ayuda de otro modelo pre-entrenado de Haar Cascade que nos permitirá reconocerlos y así mostrarlos en tiempo real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Q8viZU-IJn"
      },
      "source": [
        "# JavaScript para crear correctamente nuestro flujo de vídeo en directo utilizando nuestra cámara web como entrada\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No existen datos';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: green; font-weight: bold;\">' +\n",
        "          'Cuando haya terminado, haga clic aquí o en el vídeo para detener esta demostración</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox, bbox_eye_left, bbox_eye_right):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\", \"{}\")'.format(label, bbox, bbox_eye_left, bbox_eye_right))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWP11w8708S"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Mostrar el nombre de la persona solamente\n",
        "imagePaths = os.listdir(dataPath)\n",
        "name = imagePaths[0]\n",
        "\n",
        "# Iniciar la transmisión de vídeo desde la cámara web\n",
        "video_stream()\n",
        "# Etiqueta para el vídeo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que esté vacío\n",
        "bbox = ''\n",
        "bbox_eye_left = ''\n",
        "bbox_eye_right = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox, bbox_eye_left, bbox_eye_right)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposición transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "    bbox_array_eye_left = bbox_array\n",
        "    bbox_array_eye_right = bbox_array\n",
        "\n",
        "    # Imagen en escala de grises para la detección de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la región de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposición\n",
        "    for (x,y,w,h) in faces:\n",
        "      # Obtiene el área donde se ubica el rostro \n",
        "      result = face_recognizer.predict(gray)\n",
        "      # Muestra el resultado en una escala definida para saber que tan probable es que ese sea el rostro de la persona almacenada\n",
        "      cv2.putText(bbox_array,'{}'.format(result),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
        "      # Si está dentro del rango de certeza muestra el nombre de la persona que se considera puede ser\n",
        "      if result[1] < certain:        \n",
        "          cv2.putText(bbox_array,'{}'.format(name),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      # Si no está dentro del rango simplemente lo muestra como persona desconocida\n",
        "      else:  \n",
        "          cv2.putText(bbox_array,'Desconocido',(x,y-20),2,0.8,(219,68,55),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(219,68,55),2)\n",
        "\n",
        "    # Obtener las coordenadas de la región del ojo izquierdo\n",
        "    eye_left = eye_left_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de los ojos para la superposición\n",
        "    for (x,y,w,h) in eye_left:\n",
        "      bbox_array_eye_left = cv2.rectangle(bbox_array_eye_left,(x,y),(x+w,y+h),(50, 100, 0),2)\n",
        "\n",
        "    # Obtener las coordenadas de la región del ojo izquierdo\n",
        "    eye_right = eye_right_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de los ojos para la superposición\n",
        "    for (x,y,w,h) in eye_right:\n",
        "      bbox_array_eye_right = cv2.rectangle(bbox_array_eye_right,(x,y),(x+w,y+h),(50, 100, 0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_array_eye_left[:,:,3] = bbox_array[:,:,3]\n",
        "    bbox_array_eye_right[:,:,3] = bbox_array[:,:,3]\n",
        "\n",
        "    # Convertir la superposición de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox_bytes_eye_left = bbox_eye_left_to_bytes(bbox_array_eye_left)\n",
        "    bbox_bytes_eye_right = bbox_eye_right_to_bytes(bbox_array_eye_right)\n",
        "\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposición\n",
        "    bbox = bbox_bytes\n",
        "    bbox_eye_left = bbox_bytes_eye_left\n",
        "    bbox_eye_right = bbox_bytes_eye_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63I-82FIt_VR"
      },
      "source": [
        "# **Detección De Mentiras** (En construcción)\n",
        "\n",
        "El algoritmo trata de sacar un punto promedio, este es el centro de la pupila que se obtiene de un **\"entrenamiento\"** previo (con el ojo estático), después del entrenamiento se hace una resta del punto centro de la pupila detectada menos el punto promedio y si el resultado es muy grande (sobrepasa un umbral) significa que el ojo esta en otra posición = NOS MIENTE.\n",
        "\n",
        "## TODO\n",
        "1. Detectar bien los círculos (pupilas).\n",
        "2. Dimensionar la imagen original para mayor velocidad de procesamiento.\n",
        "3. Hacer los radios de los círculos adaptativos dependiendo del área que ocupan los ojos.\n",
        "4. Detectar dilatación. (Mejora)\n",
        "5. Agregar marcas de tiempo para establecer los momentos que el sujeto pueda estar mintiendo o no. (Mejora)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWUcZf-qH14W"
      },
      "source": [
        "print(\"%s: Todavía no sé reconocer cuando me mienten\" % now)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
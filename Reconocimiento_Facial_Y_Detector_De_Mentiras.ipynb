{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reconocimiento Facial Y Detector De Mentiras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7mg/n+qJ2AFTnr5rthnWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b878db24636047ca99a6bfd9e7fdd4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34cb1df079734b699e3e5ee75a0982a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95eb67dfa563452ca247d5a82f19432e",
              "IPY_MODEL_d9c6ac2d653b46f6bed02f7dba5206bc"
            ]
          }
        },
        "34cb1df079734b699e3e5ee75a0982a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95eb67dfa563452ca247d5a82f19432e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e89ff006e20d4bb29154a39d5f35e04c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a889e2e567f1472f8b87aff160514c5a"
          }
        },
        "d9c6ac2d653b46f6bed02f7dba5206bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a5a2197dd794812b5e554b5782eb1b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:00&lt;00:00, 4138.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1df4564e1cc14d9e9e3c06a68cc1073a"
          }
        },
        "e89ff006e20d4bb29154a39d5f35e04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a889e2e567f1472f8b87aff160514c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a5a2197dd794812b5e554b5782eb1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1df4564e1cc14d9e9e3c06a68cc1073a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank14/Reconocimiento_Facial_Y_Detector_De_Mentiras/blob/main/Reconocimiento_Facial_Y_Detector_De_Mentiras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hnQ5GxAvAzj"
      },
      "source": [
        "# **Sistema detector de mentiras**\n",
        "\n",
        "El presente proyecto utiliza OpenCV y Haar Cascade para la detecciÃ³n y clasificaciÃ³n de rostros.\n",
        "\n",
        "Para el reconocimiento facial emplearemos: Eigenfaces y el Local Binary Patterns Histograms.\n",
        "\n",
        "Y finalmente para la detecciÃ³n de mentiras se usan los modelos pre-entredados para detectar las pupilas y los ojos para determinar por medio de un simple cÃ¡lculo, donde dependiendo la posiciÃ³n de los ojos en cuanto a la posiciÃ³n inicial menos la final la posibilidad de estar mintiendo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PIwMdv5jzz4"
      },
      "source": [
        "# **Acceso a la cÃ¡mara web para obtener imÃ¡genes y vÃ­deo**\n",
        "\n",
        "Este cuaderno repasarÃ¡ cÃ³mo acceder y ejecutar cÃ³digo en imÃ¡genes y vÃ­deos tomados con tu webcam.\n",
        "\n",
        "Para el propÃ³sito de este proyecto vamos a utilizar **Haar Cascade** de **OpenCV** para hacer la detecciÃ³n de rostros capturados a travÃ©s de la cÃ¡mara de nuestra computadora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN99l9wff_Yb"
      },
      "source": [
        "# Importar dependencias\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from time import time\n",
        "import datetime\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hNpJ4gk2Cq"
      },
      "source": [
        "# **Funciones de ayuda**\n",
        "\n",
        "A continuaciÃ³n se presentan algunas funciones de ayuda para realizar la conversiÃ³n entre diferentes tipos y formatos de datos de imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PJHsiQPlCNH"
      },
      "source": [
        "# FunciÃ³n para convertir el objeto JavaScript en una imagen OpenCV\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: Objeto JavaScript que contiene la imagen de la cÃ¡mara web\n",
        "  Returns:\n",
        "          img: OpenCV BGR imagen\n",
        "  \"\"\"\n",
        "  # Decodificar imagen base64\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # Convertir bytes a array numpy\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype = np.uint8)\n",
        "  # Decodificar la matriz numpy en la imagen BGR de OpenCV\n",
        "  img = cv2.imdecode(jpg_as_np, flags = 1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# FunciÃ³n para convertir la imagen del cuadro delimitador del rectÃ¡ngulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vÃ­deo\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (pÃ­xeles) que contiene el rectÃ¡ngulo a superponer en el flujo de vÃ­deo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "# FunciÃ³n para convertir la imagen del cuadro del ojo izquierdo delimitador del rectÃ¡ngulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vÃ­deo\n",
        "def bbox_eye_left_to_bytes(bbox_array_eye_left):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (pÃ­xeles) que contiene el rectÃ¡ngulo a superponer en el flujo de vÃ­deo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_eye_left_PIL = PIL.Image.fromarray(bbox_array_eye_left, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_eye_left_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes_eye_left = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes_eye_left\n",
        "\n",
        "# FunciÃ³n para convertir la imagen del cuadro del ojo derecho delimitador del rectÃ¡ngulo de OpenCV en una cadena de bytes base64 para superponerla a la secuencia de vÃ­deo\n",
        "def bbox_eye_right_to_bytes(bbox_array_eye_right):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Matriz Numpy (pÃ­xeles) que contiene el rectÃ¡ngulo a superponer en el flujo de vÃ­deo.\n",
        "  Returns:\n",
        "        bytes: Cadena de bytes de imagen Base64\n",
        "  \"\"\"\n",
        "  # Convertir la matriz en imagen PIL\n",
        "  bbox_eye_right_PIL = PIL.Image.fromarray(bbox_array_eye_right, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # Formatear bbox para retornar en formato png\n",
        "  bbox_eye_right_PIL.save(iobuf, format = 'png')\n",
        "  # Format resultado resgresado como una cadena\n",
        "  bbox_bytes_eye_right = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes_eye_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91e2NgVEmiDO"
      },
      "source": [
        "# **Haar Cascade Classifier**\n",
        "\n",
        "Para este proyecto vamos a ejecutar un algoritmo simple de detecciÃ³n de objetos llamado **Haar Cascade** en nuestras imÃ¡genes y vÃ­deo obtenido de nuestra cÃ¡mara web. **OpenCV** tiene un modelo de detecciÃ³n de rostros pre-entrenado en **Haar Cascade**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzwYU1dm-3L"
      },
      "source": [
        "# Inicializar el modelo de detecciÃ³n de rostros y ojos en Haar Cascade\n",
        "# Entre mejor sean los modelos, mejor serÃ¡n los resultados\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'))\n",
        "eye_left_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_lefteye_2splits.xml'))\n",
        "eye_right_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_righteye_2splits.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02b-1H0vnZq8"
      },
      "source": [
        "# **ImÃ¡genes de la cÃ¡mara web**\n",
        "\n",
        "Ejecutar cÃ³digo en imÃ¡genes tomadas de la cÃ¡mara web es bastante sencillo. Utilizaremos cÃ³digo dentro de **Google Colab's Code Snippets** que tiene una variedad de funciones de cÃ³digo Ãºtiles para realizar varias tareas.\n",
        "\n",
        "Utilizaremos el fragmento de cÃ³digo de **Camera Capture** para utilizar la cÃ¡mara web de tu computadora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OJ72vZSn396"
      },
      "source": [
        "def take_photo(filename = 'photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Tomar Foto';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Redimensiona la salida para que se ajuste al elemento de vÃ­deo.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Espere a que se haga clic en Capturar.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # Obtener datos fotogrÃ¡ficos\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # Obtener una imagen en formato OpenCV\n",
        "  img = js_to_image(data) \n",
        "  # Imagen en escala de grises\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # Obtener las coordenadas de la caja delimitadora de la cara mediante la cascada de Haar\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # Dibujar el cuadro delimitador de la cara en la imagen\n",
        "  for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(128, 255, 0),2)\n",
        "  # Guardar la imagen\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X74gYZZeS4kt"
      },
      "source": [
        "## **Ejecutar la captura con la camara**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BAfGdvdoPoX"
      },
      "source": [
        "try:\n",
        "  # Nombre del archivo a guardar\n",
        "  filename = take_photo('photo.jpg')\n",
        "  # Mostrando por consola el nombre del archivo capturado\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Muestra la imagen que se acaba de tomar\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Se lanzarÃ¡n errores si el usuario no tiene una cÃ¡mara web \n",
        "  # O si no concede el permiso acceso al uso de la misma\n",
        "  print('Se presenta el siguiente error: ' + str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXn5HNmSqzZ2"
      },
      "source": [
        "# **VÃ­deo de cÃ¡mara web**\n",
        "\n",
        "Ejecutar el cÃ³digo en el vÃ­deo de la cÃ¡mara web es un poco mÃ¡s complejo que las imÃ¡genes. Tenemos que iniciar un flujo de vÃ­deo utilizando nuestra cÃ¡mara web como entrada. A continuaciÃ³n, ejecutamos cada fotograma a travÃ©s de nuestro programa **(detecciÃ³n de rostros)** y creamos una imagen superpuesta que contiene el cuadro delimitador de la detecciÃ³n. A continuaciÃ³n, superponemos la imagen del cuadro delimitador en el siguiente fotograma de nuestro flujo de vÃ­deo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giNPTpJPsXz3"
      },
      "source": [
        "# JavaScript para crear correctamente nuestro flujo de vÃ­deo en directo utilizando nuestra cÃ¡mara web como entrada\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No existen datos';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: green; font-weight: bold;\">' +\n",
        "          'Cuando haya terminado, haga clic aquÃ­ o en el vÃ­deo para detener esta demostraciÃ³n</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNRKJ6aW22E"
      },
      "source": [
        "## **Iniciar la transmisiÃ³n de vÃ­deo desde la cÃ¡mara web**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dADWXnDqs_3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1fc7a757-c975-42e2-a11d-b8e4ae872b14"
      },
      "source": [
        "# Iniciar la transmisiÃ³n de vÃ­deo desde la cÃ¡mara web\n",
        "video_stream()\n",
        "# Etiqueta para el vÃ­deo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que estÃ© vacÃ­o\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposiciÃ³n transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "\n",
        "    # Imagen en escala de grises para la detecciÃ³n de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la regiÃ³n de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposiciÃ³n\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(128,255,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # Convertir la superposiciÃ³n de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposiciÃ³n\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No existen datos';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: green; font-weight: bold;\">' +\n",
              "          'Cuando haya terminado, haga clic aquÃ­ o en el vÃ­deo para detener esta demostraciÃ³n</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzhKqyPtvRK"
      },
      "source": [
        "# **Reconocimiento Facial**\n",
        "\n",
        "Para poder llevar a cabo el reconocimiento facial, en primer lugar necesitaremos recolectar los datos, es decir los rostros de las personas que se desee reconocer, posteriormente entrenaremos el clasificador, para finalmente probarlo. Para todo este proceso serÃ¡ necesario usar la  detecciÃ³n de rostros con **haarcascades** que habÃ­amos visto anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu-UM-btthVw"
      },
      "source": [
        "## **Almacenar datos de personas manualmente**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKIWFA9L7kvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fde069-9928-486b-c11c-eb0712f24da6"
      },
      "source": [
        "# Se solicita el nombre de la persona a la que se realizarÃ¡ el reconocimiento\n",
        "personName = input('Nombre de la persona a reconocer: ')\n",
        "# Se crea la carpeta donde se almacenaran las imagenes\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "personPath = dataPath + '/' + personName\n",
        "# Si no existe la carpeta se crea\n",
        "if not os.path.exists(personPath):\n",
        "  print('Carpeta creada: ',personPath)\n",
        "  os.makedirs(personPath)\n",
        "\n",
        "def take_photo(filename = dataPath + '/' + personName, quality=0.8):\n",
        "  js = Javascript('''\n",
        "    function delay(n){\n",
        "    return new Promise(function(resolve){\n",
        "        setTimeout(resolve,n*1000);\n",
        "    });\n",
        "    } \n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Redimensiona la salida para que se ajuste al elemento de vÃ­deo.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Espera de 5 segundos para que el obturador capture correctamente la imagen.\n",
        "      await delay(5);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # Obtener datos fotogrÃ¡ficos\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # Obtener una imagen en formato OpenCV\n",
        "  img = js_to_image(data) \n",
        "  # Imagen en escala de grises\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # Obtener las coordenadas de la caja delimitadora de la cara mediante la cascada de Haar\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # Dibujar el cuadro delimitador de la cara en la imagen\n",
        "  for (x,y,w,h) in faces:\n",
        "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(128, 255, 0),2)\n",
        "  # Guardar la imagen\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de la persona a reconocer: Francisco\n",
            "Carpeta creada:  Reconocimiento Facial/Data/Francisco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVME6VFtBG58"
      },
      "source": [
        "# Habilitar capturas por video\n",
        "cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
        "# Inicializando el contador para colocar diferentes nombres en las imÃ¡genes\n",
        "count = 0\n",
        "# Se conculta al usuario la cantidad de fotos a tomar se recomiendan pocas para pruebas rÃ¡pidas\n",
        "photo = int(input('Ingrese la cantidad de imagenes a capturar: '))\n",
        "# Se ejecutara el ciclo segÃºn la cantidad indicada por el usuario\n",
        "\n",
        "while True:\n",
        "  # Leyendo las capturas\n",
        "  ret, frame = cap.read()\n",
        "  try:\n",
        "    # Nombre y ruta para almacenar las imÃ¡genes de entrenamiento\n",
        "    filename = take_photo(personPath + '/' + 'rostro_{}.jpg'.format(count + 1))\n",
        "    # Incremetando en 1 el contador por cada imÃ¡gen capturada\n",
        "    count = count + 1\n",
        "    # Mostrando por consola el nombre del archivo capturado\n",
        "    print('Saved to {}'.format(filename))\n",
        "    # Tiempo de espera de 1 ms entre foto y foto\n",
        "    k =  cv2.waitKey(1)\n",
        "    # CondiciÃ³n para finalizar el ciclo\n",
        "    if k == 27 or count >= photo:\n",
        "        break\n",
        "    # Muestra la imagen que se acaba de tomar\n",
        "    display(Image(filename))\n",
        "  except Exception as err:\n",
        "    # Se lanzarÃ¡n errores si el usuario no tiene una cÃ¡mara web \n",
        "    # O si no concede el permiso acceso al uso de la misma\n",
        "    print('Se presenta el siguiente error: ' + str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgCVJE1ptZ38"
      },
      "source": [
        "## **Descargar datos de reconocimiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "b878db24636047ca99a6bfd9e7fdd4f4",
            "34cb1df079734b699e3e5ee75a0982a8",
            "95eb67dfa563452ca247d5a82f19432e",
            "d9c6ac2d653b46f6bed02f7dba5206bc",
            "e89ff006e20d4bb29154a39d5f35e04c",
            "a889e2e567f1472f8b87aff160514c5a",
            "9a5a2197dd794812b5e554b5782eb1b1",
            "1df4564e1cc14d9e9e3c06a68cc1073a"
          ]
        },
        "id": "MS_TUKmJtXXo",
        "outputId": "ee408a78-23ca-48c0-bb1e-35a04a583f48"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "from multiprocessing.pool import ThreadPool\n",
        "\n",
        "def url_response(url):\n",
        "    path, url = url\n",
        "    r = requests.get(url, stream = True)\n",
        "    with open(path, 'wb') as f:\n",
        "        for ch in r:\n",
        "            f.write(ch)\n",
        "\n",
        "# Se solicita el nombre de la persona a la que se realizarÃ¡ el reconocimiento\n",
        "personName = input('Nombre de la persona a reconocer: ')\n",
        "# Se crea la carpeta donde se almacenaran las imagenes\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "personPath = dataPath + '/' + personName + '/'\n",
        "# Si no existe la carpeta se crea\n",
        "if not os.path.exists(personPath):\n",
        "  print('Carpeta creada: ',personPath)\n",
        "  os.makedirs(personPath)\n",
        "\n",
        "urls = [(personPath + \"Event1\", \"https://www.python.org/events/python-events/805/\"),\n",
        "(personPath + \"Event2\", \"https://www.python.org/events/python-events/801/\"),\n",
        "(personPath + \"Event3\", \"https://www.python.org/events/python-events/790/\"),\n",
        "(personPath + \"Event4\", \"https://www.python.org/events/python-events/798/\"),\n",
        "(personPath + \"Event5\", \"https://www.python.org/events/python-events/807/\"),\n",
        "(personPath + \"Event6\", \"https://www.python.org/events/python-events/807/\"),\n",
        "(personPath + \"Event7\", \"https://www.python.org/events/python-events/757/\"),\n",
        "(personPath + \"Event8\", \"https://www.python.org/events/python-user-group/816/\")]\n",
        "\n",
        "ThreadPool(9).imap_unordered(url_response, urls)\n",
        "\n",
        "print(\"Descargando imagenes...\")\n",
        "for i in tqdm(range(1000)):\n",
        "  print(\"\", end = '\\r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de la persona a reconocer: Lorena\n",
            "Carpeta creada:  Reconocimiento Facial/Data/Lorena/\n",
            "Descargando imagenes...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b878db24636047ca99a6bfd9e7fdd4f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUdHMsAmFkBj"
      },
      "source": [
        "## **Preparando Los Datos Para Entrenar**\n",
        "\n",
        "Antes de proceder con el entrenamiento es necesario tener cada una de las imÃ¡genes con una etiqueta asociada a la persona a la que pertenecen los rostros. Por ello, por ejemplo cuando leamos la carpeta **â€˜Persona 1â€™** todas esas imÃ¡genes se les asignarÃ¡ etiqueta 0, luego a todas imÃ¡genes de los rostros de **â€˜Persona 2â€™** se asignarÃ¡ 1,  de **â€˜Persona 3â€™** se asignarÃ¡ 2, y asÃ­ sucesivamente. Con cada una de estas etiquetas le haremos saber al computador que las imÃ¡genes le corresponden a personas diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFAx7AjqF8g5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6411374a-223b-45ef-8dc0-6db43eaa59c8"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Se listan las personas por carpetas\n",
        "peopleList = os.listdir(dataPath)\n",
        "# Se imprime el listado de personas en el sistema\n",
        "print('Lista de personas: ', peopleList)\n",
        "# Declaramos labels en este se almacenarÃ¡n las etiquetas correspondientes a cada imagen segÃºn la persona\n",
        "labels = []\n",
        "# Declaramos facesData en donde se almacenarÃ¡ cada una de las imÃ¡genes de los rostros\n",
        "facesData = []\n",
        "# Estableceremos un contador label en 0, para conforme se termine de leer las imÃ¡genes de una persona, cambie a otro valor. Esto ayudarÃ¡ al clasificador a entender que se tarta de diferentes personas\n",
        "label = 0\n",
        "for nameDir in peopleList:\n",
        "    personPath = dataPath + '/' + nameDir\n",
        "    print('Leyendo las imÃ¡genes')\n",
        "    for fileName in os.listdir(personPath):\n",
        "        print('Rostros: ', nameDir + '/' + fileName)\n",
        "        labels.append(label)\n",
        "        facesData.append(cv2.imread(personPath+'/'+fileName,0))\n",
        "    label = label + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lista de personas:  ['RaulEvent1', '.ipynb_checkpoints', 'RaulEvent6', 'RaulEvent7', 'Raul', 'RaulEvent3', 'RaulEvent4', 'RaulEvent8', 'RaulEvent2', 'Francisco', 'RaulEvent5', 'Lorena']\n",
            "Leyendo las imÃ¡genes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-8a412abba04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpersonPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnameDir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Leyendo las imÃ¡genes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rostros: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnameDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'Reconocimiento Facial/Data/RaulEvent1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfZftZEZLvES"
      },
      "source": [
        "## **MÃ©todos para entrenar el reconocedor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLEvPsgg8L4y"
      },
      "source": [
        "#@title **Seleccione una de las siguientes opciones:** { display-mode: \"form\", run: \"auto\"}\n",
        "opciones = \"EigenFaces\" #@param [\"EigenFaces\", \"LBPH\"]\n",
        "\n",
        "print(\"La opciÃ³n seleccionada fue: \" + opciones)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnic2jbRMNva"
      },
      "source": [
        "# MÃ©todos para entrenar el reconocedor\n",
        "# Estos mÃ©todos crean la instancia, escriben y leen el set de datos para poder reconocer las imagenes solicitadas\n",
        "if opciones == \"EigenFaces\":\n",
        "  face_recognizer = cv2.face.EigenFaceRecognizer_create()\n",
        "  face_recognizer.write('modeloEigenFace.xml')\n",
        "  face_recognizer.read('modeloEigenFace.xml')\n",
        "  certain = 5700\n",
        "if opciones == \"LBPH\":\n",
        "  face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "  face_recognizer.write('modeloLBPHFace.xml')\n",
        "  face_recognizer.read('modeloLBPHFace.xml')\n",
        "  certain = 70\n",
        "else:\n",
        "  print(\"La opciÃ³n seleccionada no es valida\")\n",
        "\n",
        "# Entrenando el reconocedor de rostros\n",
        "face_recognizer.train(facesData, np.array(labels))\n",
        "\n",
        "# Barra de carga para una espera mÃ¡s agradable\n",
        "print(\"Entrenando...\")\n",
        "for i in tqdm(range(10000)):\n",
        "  print(\"\", end = '\\r')\n",
        "print(\"Entrenamiento completado al 100%\")\n",
        "print()\n",
        "print(\"Modelo almacenado exitosamente\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSOwIAMsM3EV"
      },
      "source": [
        "## **Probando EigenFaces y LBPH para el reconocimiento facial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18SEo30NAd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5cf097e4-75c4-4397-d83f-e55093211b80"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Mostrar el nombre de la persona solamente\n",
        "imagePaths = os.listdir(dataPath)\n",
        "name = imagePaths[0]\n",
        "\n",
        "# Iniciar la transmisiÃ³n de vÃ­deo desde la cÃ¡mara web\n",
        "video_stream()\n",
        "# Etiqueta para el vÃ­deo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que estÃ© vacÃ­o\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposiciÃ³n transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "\n",
        "    # Imagen en escala de grises para la detecciÃ³n de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la regiÃ³n de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposiciÃ³n\n",
        "    for (x,y,w,h) in faces:\n",
        "      # Obtiene el Ã¡rea donde se ubica el rostro \n",
        "      result = face_recognizer.predict(gray)\n",
        "      # Muestra el resultado en una escala definida para saber que tan probable es que ese sea el rostro de la persona almacenada\n",
        "      cv2.putText(bbox_array,'{}'.format(result),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
        "      # Si estÃ¡ dentro del rango de certeza muestra el nombre de la persona que se considera puede ser\n",
        "      if result[1] < certain:        \n",
        "          cv2.putText(bbox_array,'{}'.format(name),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      # Si no estÃ¡ dentro del rango simplemente lo muestra como persona desconocida\n",
        "      else:  \n",
        "          cv2.putText(bbox_array,'Desconocido',(x,y-20),2,0.8,(219,68,55),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(219,68,55),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # Convertir la superposiciÃ³n de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposiciÃ³n\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No existen datos';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: green; font-weight: bold;\">' +\n",
              "          'Cuando haya terminado, haga clic aquÃ­ o en el vÃ­deo para detener esta demostraciÃ³n</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQCiYAUo00eC"
      },
      "source": [
        "---\n",
        "# **DetecciÃ³n de mentiras**\n",
        "\n",
        "Para este proyecto el objetivo fue detectar las iris de las personas, esto con el fin de detectar mentiras segÃºn la posiciÃ³n.\n",
        "\n",
        "\"*Los ojos nos hablan, pero a veces no sabemos interpretar lo que nos estÃ¡n diciendo. Con un entrenamiento adecuado podremos detectar hasta el mÃ¡s sutil de los detalles.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XelhmeRvt3Lu"
      },
      "source": [
        "# **DetecciÃ³n De Ojos**\n",
        "\n",
        "Para la detecciÃ³n de ojos simplemente reutilizamos el cÃ³digo anterior del video y de funciones de ayuda para que el recuadro de los ojos se guarde como una imagen los cuales serÃ¡n capturados con ayuda de otro modelo pre-entrenado de Haar Cascade que nos permitirÃ¡ reconocerlos y asÃ­ mostrarlos en tiempo real."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Q8viZU-IJn"
      },
      "source": [
        "# JavaScript para crear correctamente nuestro flujo de vÃ­deo en directo utilizando nuestra cÃ¡mara web como entrada\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Estado:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No existen datos';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: green; font-weight: bold;\">' +\n",
        "          'Cuando haya terminado, haga clic aquÃ­ o en el vÃ­deo para detener esta demostraciÃ³n</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox, bbox_eye_left, bbox_eye_right):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\", \"{}\", \"{}\")'.format(label, bbox, bbox_eye_left, bbox_eye_right))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWP11w8708S"
      },
      "source": [
        "# Se indica la carpeta donde se almacenaron las imagenes de entrenamiento\n",
        "dataPath = 'Reconocimiento Facial/Data'\n",
        "# Mostrar el nombre de la persona solamente\n",
        "imagePaths = os.listdir(dataPath)\n",
        "name = imagePaths[0]\n",
        "\n",
        "# Iniciar la transmisiÃ³n de vÃ­deo desde la cÃ¡mara web\n",
        "video_stream()\n",
        "# Etiqueta para el vÃ­deo\n",
        "label_html = 'Capturando...'\n",
        "# Inicializar el cuadro delimitador para que estÃ© vacÃ­o\n",
        "bbox = ''\n",
        "bbox_eye_left = ''\n",
        "bbox_eye_right = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox, bbox_eye_left, bbox_eye_right)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convertir la respuesta JS en imagen OpenCV\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Crear una superposiciÃ³n transparente para el cuadro delimitador\n",
        "    bbox_array = np.zeros([480,640,4], dtype = np.uint8)\n",
        "    bbox_array_eye_left = bbox_array\n",
        "    bbox_array_eye_right = bbox_array\n",
        "\n",
        "    # Imagen en escala de grises para la detecciÃ³n de rostros\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Obtener las coordenadas de la regiÃ³n de la cara\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de la cara para la superposiciÃ³n\n",
        "    for (x,y,w,h) in faces:\n",
        "      # Obtiene el Ã¡rea donde se ubica el rostro \n",
        "      result = face_recognizer.predict(gray)\n",
        "      # Muestra el resultado en una escala definida para saber que tan probable es que ese sea el rostro de la persona almacenada\n",
        "      cv2.putText(bbox_array,'{}'.format(result),(x,y-5),1,1.3,(255,255,0),1,cv2.LINE_AA)\n",
        "      # Si estÃ¡ dentro del rango de certeza muestra el nombre de la persona que se considera puede ser\n",
        "      if result[1] < certain:        \n",
        "          cv2.putText(bbox_array,'{}'.format(name),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "      # Si no estÃ¡ dentro del rango simplemente lo muestra como persona desconocida\n",
        "      else:  \n",
        "          cv2.putText(bbox_array,'Desconocido',(x,y-20),2,0.8,(219,68,55),1,cv2.LINE_AA)\n",
        "          bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(219,68,55),2)\n",
        "\n",
        "    # Obtener las coordenadas de la regiÃ³n del ojo izquierdo\n",
        "    eye_left = eye_left_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de los ojos para la superposiciÃ³n\n",
        "    for (x,y,w,h) in eye_left:\n",
        "      bbox_array_eye_left = cv2.rectangle(bbox_array_eye_left,(x,y),(x+w,y+h),(50, 100, 0),2)\n",
        "\n",
        "    # Obtener las coordenadas de la regiÃ³n del ojo izquierdo\n",
        "    eye_right = eye_right_cascade.detectMultiScale(gray)\n",
        "    # Obtener el cuadro delimitador de los ojos para la superposiciÃ³n\n",
        "    for (x,y,w,h) in eye_right:\n",
        "      bbox_array_eye_right = cv2.rectangle(bbox_array_eye_right,(x,y),(x+w,y+h),(50, 100, 0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_array_eye_left[:,:,3] = bbox_array[:,:,3]\n",
        "    bbox_array_eye_right[:,:,3] = bbox_array[:,:,3]\n",
        "\n",
        "    # Convertir la superposiciÃ³n de bbox en bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox_bytes_eye_left = bbox_eye_left_to_bytes(bbox_array_eye_left)\n",
        "    bbox_bytes_eye_right = bbox_eye_right_to_bytes(bbox_array_eye_right)\n",
        "\n",
        "    # Actualizar bbox para que el siguiente cuadro tenga una nueva superposiciÃ³n\n",
        "    bbox = bbox_bytes\n",
        "    bbox_eye_left = bbox_bytes_eye_left\n",
        "    bbox_eye_right = bbox_bytes_eye_right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63I-82FIt_VR"
      },
      "source": [
        "# **DetecciÃ³n De Mentiras** (En construcciÃ³n)\n",
        "\n",
        "El algoritmo trata de sacar un punto promedio, este es el centro de la pupila que se obtiene de un **\"entrenamiento\"** previo (con el ojo estÃ¡tico), despuÃ©s del entrenamiento se hace una resta del punto centro de la pupila detectada menos el punto promedio y si el resultado es muy grande (sobrepasa un umbral) significa que el ojo esta en otra posiciÃ³n = NOS MIENTE.\n",
        "\n",
        "## TODO\n",
        "1. Detectar bien los cÃ­rculos (pupilas).\n",
        "2. Dimensionar la imagen original para mayor velocidad de procesamiento.\n",
        "3. Hacer los radios de los cÃ­rculos adaptativos dependiendo del Ã¡rea que ocupan los ojos.\n",
        "4. Detectar dilataciÃ³n. (Mejora)\n",
        "5. Agregar marcas de tiempo para establecer los momentos que el sujeto pueda estar mintiendo o no. (Mejora)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWUcZf-qH14W"
      },
      "source": [
        "print(\"%s: TodavÃ­a no sÃ© reconocer cuando me mienten\" % now)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}